{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "391af31d",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "Natural Language Processing Fundamentals\n",
    "\n",
    "1. NLP Introduction\n",
    "2. NLP Stop Words\n",
    "\n",
    "Text Processing Pipeline\n",
    "\n",
    "3. POS Tagging\n",
    "4. Named Entity Recognition\n",
    "5. NLP Statistical Methods - Bag Of Words and TF-IDF\n",
    "\n",
    "Feature Engineering in NLP\n",
    "\n",
    "6. Text Normalization and Tokenization\n",
    "7. Embedding and Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60653357",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84540303",
   "metadata": {},
   "source": [
    "# 1. NLP Introduction\n",
    "\n",
    "<img src='n4.png' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a16420",
   "metadata": {},
   "outputs": [],
   "source": [
    "NLP\n",
    "\n",
    "bridge the gap between human and a machine\n",
    "\n",
    "- NLU (Natural Language Understanding) - Semantic Analytics (context and intent)\n",
    "- NLG (Natural Language Generation) - generating next word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb9c762",
   "metadata": {},
   "outputs": [],
   "source": [
    "Statistical Modeling\n",
    "\n",
    "- generative modeling - predicting something based on the probability of previous word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84b0759",
   "metadata": {},
   "outputs": [],
   "source": [
    "context\n",
    "\n",
    "\"your t-shirt is killer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1134780",
   "metadata": {},
   "outputs": [],
   "source": [
    "intent\n",
    "\n",
    "\"my mom gave me money to buy 1kg tomatos otherwise she will be angry\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92628370",
   "metadata": {},
   "outputs": [],
   "source": [
    "give instructions to a machine\n",
    "- python\n",
    "- java\n",
    "- c\n",
    "\n",
    "chatGPT\n",
    "- english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21603a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "applications of NLP\n",
    "\n",
    "- sentiment analysis\n",
    "- toxicity classification - threats, insults, hatred\n",
    "- machine translation\n",
    "- NER - Named Entity Recognition - name, organization, location or quantities\n",
    "- email spam detection\n",
    "- text generation - autocomplete, chatbots\n",
    "- information retrieval\n",
    "- summarization\n",
    "- questions and answering bots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c931f22f",
   "metadata": {},
   "source": [
    "# 2. NLP Stop Words\n",
    "\n",
    "<img src='v41.png' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cc03c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop words - on, the, of, and, with, a, \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac884f7",
   "metadata": {},
   "source": [
    "# Text Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391ca506",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"A\" = 65  # ASCII, utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0864244b",
   "metadata": {},
   "outputs": [],
   "source": [
    "you cannot feed internet data to train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d32810",
   "metadata": {},
   "outputs": [],
   "source": [
    "India != INDIA   # the data is not normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7243bde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw text\n",
    "\n",
    "\"<SUBJECT LINE> Employees details. \\\n",
    "<END><BODY TEXT>Attached are 2 files 1st, one is pairoll 2nd is healtcare !\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a66109c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove encodings\n",
    "\n",
    "\"Employees details. Attached are 2 files 1st, one is pairoll 2nd is healtcare !\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9f5010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower casing\n",
    "\n",
    "\"employees details. attached are 2 files 1st, one is pairoll 2nd is healtcare !\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8cece6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# digits to words\n",
    "\n",
    "\"employees details. attached are two files first, one is pairoll second is healtcare !\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc3531f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove special characters - @!#$%^\n",
    "\n",
    "\"employees details attached are two files first one is pairoll second is healtcare\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd23adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spelling corrections\n",
    "\n",
    "\"employees details attached are two files first one is payroll second is healthcare\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a6602e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words\n",
    "\n",
    "\"employees details attached two files first one payroll second healthcare\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9432de96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming\n",
    "\n",
    "\"employe detail attached two file first one payroll second healthcare\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2a3e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatization - ran->run, jumped->jump\n",
    "\n",
    "\"employe detail attach two file first one payroll second healthcare\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b161ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Now the text is ready to feed into a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddc727b",
   "metadata": {},
   "source": [
    "# 3. POS Tagging\n",
    "\n",
    "<img src='n5.png' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6197e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "parts of speech tagging\n",
    "\n",
    "NLTK (Natural Language Tool Kit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6b067a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318ea3a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b1dad01",
   "metadata": {},
   "source": [
    "# 4. Named Entity Recognition\n",
    "\n",
    "<img src='v45.jpg' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716374b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "we want specific information like name, organization, quantities, date, designation, subject etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797e3e01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d514d0bb",
   "metadata": {},
   "source": [
    "# 5. NLP Statistical Methods - Bag Of Words and TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d3cc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text to numerical embeddings\n",
    "\n",
    "1 - statistical methods\n",
    "    - BagOfWords\n",
    "    - TF-IDF\n",
    "\n",
    "2 - ML/DL based methods\n",
    "    - embeddings(lookup) methods\n",
    "    - Word2Vec -> Continuous BagOfWords\n",
    "    - transformer based architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335eb646",
   "metadata": {},
   "source": [
    "### Bag Of Words\n",
    "\n",
    "<img src='n1.png' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982e8290",
   "metadata": {},
   "outputs": [],
   "source": [
    "the cat sat on the mat\n",
    "the dog sat of the cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fbb3d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5676fce",
   "metadata": {},
   "source": [
    "### Sequential Representation\n",
    "\n",
    "<img src='n2.png' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d78ea46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fdf84ed1",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "\n",
    "- weight each word by its importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298bea87",
   "metadata": {},
   "outputs": [],
   "source": [
    "term = word\n",
    "\n",
    "\n",
    "\n",
    "TF (Term Frequency) - How important is the word in the document?\n",
    "\n",
    "\n",
    "IDF (Inverse Document Frequency) - how important is the word in the whole corpus?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd8ecd0",
   "metadata": {},
   "source": [
    "### 2 - ML/DL based methods\n",
    "    - embeddings(lookup) methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7201b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "we have 171k+ words in english\n",
    "\n",
    "we asign a number to each word\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8a36d8",
   "metadata": {},
   "source": [
    "# Feature Engineering in NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66b1907",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd29c274",
   "metadata": {},
   "source": [
    "# 6. Text Normalization and Tokenization\n",
    "\n",
    "Tokenization Example - https://platform.openai.com/tokenizer\n",
    "\n",
    "<img src='v42.png' />\n",
    "\n",
    "<img src='v43.jpg' />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d6553a",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "1. Word Tokenization\n",
    "2. Sentence Tokenization\n",
    "3. Regular Expression Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25801b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315d55a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'this is a single sentence.'\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "print(tokens) # ['this', 'is', 'a', 'single', 'sentence', '.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e9f8cd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', 'a', 'single', 'sentence']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_punctuation = [word.lower() for word in tokens if word.isalpha()]\n",
    "no_punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d421090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this is the first sentence.', 'this is the second sentence.', 'this is the document.']\n"
     ]
    }
   ],
   "source": [
    "text = 'this is the first sentence. this is the second sentence. this is the document.'\n",
    "\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b57ce1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['this', 'is', 'the', 'first', 'sentence', '.'], ['this', 'is', 'the', 'second', 'sentence', '.'], ['this', 'is', 'the', 'document', '.']]\n"
     ]
    }
   ],
   "source": [
    "print([word_tokenize(sentence) for sentence in sent_tokenize(text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c195f11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
     ]
    }
   ],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "\n",
    "print(stop_words[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3c73f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['first', 'sentence', '.', 'second', 'sentence', '.', 'document', '.']\n"
     ]
    }
   ],
   "source": [
    "text = 'this is the first sentence. this is the second sentence. this is the document.'\n",
    "\n",
    "tokens = [token for token in word_tokenize(text) if token not in stop_words]\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cef7c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d9a5160",
   "metadata": {},
   "source": [
    "# 7. Embedding and Word2Vec\n",
    "\n",
    "\n",
    "Word2Vec - https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d65d48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d2df71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadGlove(path):\n",
    "    file = open(path, 'r', encoding='utf8')\n",
    "    model = {}\n",
    "    \n",
    "    for l in file:\n",
    "        line = l.split()\n",
    "        word = line[0]\n",
    "        value = np.array([float(val) for val in line[1:]])\n",
    "        model[word] = value\n",
    "    \n",
    "    return model\n",
    "\n",
    "glove = loadGlove('glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "50a49c6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.5897  , -0.55043 , -1.0106  ,  0.41226 ,  0.57348 ,  0.23464 ,\n",
       "       -0.35773 , -1.78    ,  0.10745 ,  0.74913 ,  0.45013 ,  1.0351  ,\n",
       "        0.48348 ,  0.47954 ,  0.51908 , -0.15053 ,  0.32474 ,  1.0789  ,\n",
       "       -0.90894 ,  0.42943 , -0.56388 ,  0.69961 ,  0.13501 ,  0.16557 ,\n",
       "       -0.063592,  0.35435 ,  0.42819 ,  0.1536  , -0.47018 , -1.0935  ,\n",
       "        1.361   , -0.80821 , -0.674   ,  1.2606  ,  0.29554 ,  1.0835  ,\n",
       "        0.2444  , -1.1877  , -0.60203 , -0.068315,  0.66256 ,  0.45336 ,\n",
       "       -1.0178  ,  0.68267 , -0.20788 , -0.73393 ,  1.2597  ,  0.15425 ,\n",
       "       -0.93256 , -0.15025 ])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove['python']   # vector embedding for the word Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "700b9c89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.92803 ,  0.29096 ,  0.67837 ,  1.0444  , -0.72551 ,  2.1995  ,\n",
       "        0.88767 , -0.94782 ,  0.67426 ,  0.24908 ,  0.95722 ,  0.18122 ,\n",
       "        0.064263,  0.64323 , -1.6301  ,  0.94972 , -0.7367  ,  0.17345 ,\n",
       "        0.67638 ,  0.10026 , -0.033782, -0.76971 ,  0.40519 , -0.099516,\n",
       "        0.79654 ,  0.1103  , -0.076053, -0.090434,  0.015021, -1.137   ,\n",
       "        1.6803  , -0.34424 ,  0.77538 , -1.8718  , -0.17148 ,  0.31956 ,\n",
       "        0.093062,  0.004996,  0.25716 ,  0.52207 , -0.52548 , -0.93144 ,\n",
       "       -1.0553  ,  1.4401  ,  0.30807 , -0.84872 ,  1.9986  ,  0.10788 ,\n",
       "       -0.23633 , -0.17978 ])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove['neural']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0129d31",
   "metadata": {},
   "source": [
    "### How the system know that these words are similar?\n",
    "\n",
    "- Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bf288386",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "178263a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.92180053]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(glove['cat'].reshape(1,-1), glove['dog'].reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f968f0d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.19825255]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(glove['cat'].reshape(1,-1), glove['piano'].reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "76946819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7839043]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(glove['king'].reshape(1,-1), glove['queen'].reshape(1,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9f09d1",
   "metadata": {},
   "source": [
    "## Words in 2D Embedding Space\n",
    "\n",
    "<img src='v44.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45978ab3",
   "metadata": {},
   "source": [
    "# pipeline for text data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe21d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Text Normalization\n",
    "2. Tokenization\n",
    "3. Tokens to IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28885124",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Celebration           of Worldcup winning !!\"\n",
    "\n",
    "\n",
    "Text Normalization\n",
    "\n",
    "- lowercasing\n",
    "- puctuation removal(?, !)\n",
    "- trim whitespaces\n",
    "- strip accents\n",
    "- stemming\n",
    "- lemmatization\n",
    "\n",
    "\"celebrate of worldcup win\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccda1f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tokenization\n",
    "\n",
    "- word tokenization\n",
    "- subword tokenization\n",
    "- character tokenization\n",
    "- sentence tokenization\n",
    "- regular expression tokenization\n",
    "\n",
    "['celebrate', 'of', 'worldcup', 'win']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5629a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tokens to IDs\n",
    "\n",
    "- words to numerical values\n",
    "- lookup table -> animal - 18, car - 128\n",
    "\n",
    "- hashing - we create a function that will give us a random ID for that word\n",
    "\n",
    "[35, 21, 4, 18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ca402f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff948e53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378aec7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df39601f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
