{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df82f8ba",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "\n",
    "0.0 → Introduction to AI and its Evolution\n",
    "\n",
    "0.1 → Machine Learning vs Deep Learning\n",
    "\n",
    "0.2 → Tokens VS Parameters in Models\n",
    "\n",
    "0.3 → What can AI realistically achieve today?\n",
    "\n",
    "0.4 → 25 Papers That Completely Transformed the Computer World"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f23879",
   "metadata": {},
   "source": [
    "### 0.0 → Introduction to AI and its Evolution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc055d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "replicate human intelligence\n",
    "\n",
    "milestones\n",
    "\n",
    "1943 - A logical calculus of the ideas immanent in neurons activity - foundation of neural network\n",
    "\n",
    "1950 - turing test\n",
    "\n",
    "1956 - Artificial Intelligence - John McCarthy\n",
    "\n",
    "\n",
    "1965 - NLP\n",
    "\n",
    "1997 - IBM Deep blue defeats world champion\n",
    "\n",
    "2004 - self-driving cars\n",
    "\n",
    "2006 - deep learning - Geoffery Hinton\n",
    "\n",
    "2007 - Apple - voice recognition (siri added in 2011)\n",
    "\n",
    "2012 - AlexNet - deep CNN - computer vision\n",
    "\n",
    "2014 - Google Deep mind - AlphaGo defeats a human through reinforcement learning\n",
    "\n",
    "2017 - AlphaGo zero - just by self-play it defeats its predecessor without any human data.\n",
    "\n",
    "2020 - GPT-3\n",
    "\n",
    "2021 - Google - MUM (Multitask Unified Model)\n",
    "\n",
    "2022 - DALL-E 2 and stable diffusion bring GenerativeAI\n",
    "\n",
    "2023 - DeepMind - AlphaFold - accurately predicts the structure of all know proteins.\n",
    "\n",
    "2024 - LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fdf3f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2fe18ad6",
   "metadata": {},
   "source": [
    "### 0.1 → Machine Learning vs Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15daa185",
   "metadata": {},
   "source": [
    "How to know features without having domain knowledge?\n",
    "\n",
    "https://github.com/hemansnation/AI-ML-MLOps-GenAI-Live-Summer-Cohort-2024/blob/main/Module%2006%20-%20Machine%20Learning%20Algorithms/1_Regression%20Analysis/3_Multiple%20Linear%20Regression/8_9_Multiple%20Linear%20Regression%20Part%203.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828a9e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "Machine Learning\n",
    "\n",
    "- feature engineering\n",
    "- structured data - tabular, relational DB\n",
    "- domain knowledge for design features\n",
    "- can be trained on small data\n",
    "- CPUs\n",
    "- understanding of decision\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a425ebaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Deep Learning\n",
    "\n",
    "- no need for feature engineering\n",
    "- unstructured data - images, audio, video, text\n",
    "- minimizes the need for manual feature engineering\n",
    "- need large amounts of data to perform well\n",
    "- GPUs and TPUs\n",
    "- it consider black box and harder to explain how it come up with the prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd3eb27",
   "metadata": {},
   "source": [
    "### 0.2 → Tokens VS Parameters in Models\n",
    "\n",
    "https://newsletter.himanshuramchandani.co/p/tokens-vs-parameters-in-llms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675f6903",
   "metadata": {},
   "outputs": [],
   "source": [
    "this is an _ apple\n",
    "\n",
    "4 words \n",
    "- 4 word token\n",
    "\n",
    "16 characters\n",
    "- 16 character token\n",
    "\n",
    "sub token\n",
    "\n",
    "\n",
    "\n",
    "GPT3 \n",
    "- 500 billion word token\n",
    "- 175 Billion parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a35a87e",
   "metadata": {},
   "source": [
    "How ChatGPT works\n",
    "https://github.com/hemansnation/Generative-AI-Zero-to-Hero/blob/main/June%202024/Module%204/4_Large%20Language%20Models.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3297642e",
   "metadata": {},
   "source": [
    "### 0.3 → What can AI realistically achieve today?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be82c347",
   "metadata": {},
   "outputs": [],
   "source": [
    "NLP - chatbots and Virtual assistants\n",
    "- google assistant\n",
    "- siri\n",
    "- alexa\n",
    "\n",
    "- text generation\n",
    "- sentiment analysis\n",
    "- language translation\n",
    "\n",
    "Computer Vision\n",
    "- autonomous vehicles\n",
    "- tesla, waymo\n",
    "what a tesla car sees: https://www.youtube.com/watch?v=fKXztwtXaGo\n",
    "\n",
    "- Object detection and recognition\n",
    "- image enhancement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6b4602",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ed63169",
   "metadata": {},
   "source": [
    "### 0.4 → 25 Papers That Completely Transformed the Computer World (Transformer only)\n",
    "\n",
    "Attention Is All You Need: Into a new deep learning architecture known as the transformer\n",
    "\n",
    "https://arxiv.org/pdf/1706.03762"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a605e37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "deep learning architecture - Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36edff4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tranditional models\n",
    "- RNN\n",
    "- CNN\n",
    "- LSTM, GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a856b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "- slow\n",
    "- hard to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18f54a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention machenism\n",
    "\n",
    "Himanshu is sitting on a chair. He is going to share resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7958d979",
   "metadata": {},
   "outputs": [],
   "source": [
    "why transformers are important?\n",
    "\n",
    "- parallel processing\n",
    "- better performance\n",
    "- simple design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c44536b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT by google"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1980c6a4",
   "metadata": {},
   "source": [
    "### PyTorch Basics\n",
    "\n",
    "https://github.com/hemansnation/Machine-Learning-MLOps-GenerativeAI-NLP-CV-MLSystem-Design/blob/main/4_Module%20-%20NLP%20GenAI%20CV%20-%20Deep%20Learning/1_Natural%20Language%20Processing/1_NLP%20Fundamentals%20and%20PyTorch.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a73cad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
