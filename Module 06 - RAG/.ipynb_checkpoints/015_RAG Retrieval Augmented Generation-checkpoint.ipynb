{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Of Contents\n",
    "\n",
    "1. Introduction\n",
    "    - What is RAG?\n",
    "    - How it is different from fine-tuning?\n",
    "2. Types of RAG\n",
    "    - Naïve RAG\n",
    "    - Advanced RAG\n",
    "    - Modular RAG Retriever\n",
    "3. Applications\n",
    "4. Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Retrieval Augmented Generation - RAG\n",
    "\n",
    "- addresses limitations in LLMs\n",
    "- it will retrieves relevant data from external sources in real time\n",
    "- respond to queries that are not part of the llm training data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "    - What is RAG?\n",
    "    - How it is different from fine-tuning?\n",
    "\n",
    "<img src='l3.png' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector database\n",
    "- manage and search embedded vector representations\n",
    "\n",
    "vector libraries (FAISS, ANNOY)\n",
    "- do not have CRUD support\n",
    "- store are mostly used for static data (cannot change the index)\n",
    "- it only stores vector embeddings and not the associated objects they were generated from\n",
    "\n",
    "SQL databases\n",
    "- pgvector\n",
    "- supabase\n",
    "- starrocks\n",
    "\n",
    "full text search database\n",
    "- ElasticSearch\n",
    "- OpenSearch\n",
    "\n",
    "NoSQL databases\n",
    "- Redis\n",
    "- MongoDB\n",
    "\n",
    "Dedicated Vector databases\n",
    "- pinecone - https://docs.pinecone.io/examples/notebooks\n",
    "- milvus\n",
    "- Chroma\n",
    "- Vespa\n",
    "- Vearch\n",
    "\n",
    "ANN benchmarks - Approximate Nearest Neighbours\n",
    "\n",
    "\n",
    "these features we need to create our vector database\n",
    "- open source\n",
    "- CRUD support\n",
    "- distributed\n",
    "- replicas support\n",
    "- high scalability and performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud services building llm, we need to integrate RAG\n",
    "does the cloud provider give you RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Types of RAG\n",
    "    - Naïve RAG\n",
    "    - Advanced RAG\n",
    "    - Modular RAG Retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Naive RAG\n",
    "- doesnot invlove complex data embeddings and indexing\n",
    "- it search through keywords\n",
    "\n",
    "Advanced RAG\n",
    "- for more complex cases\n",
    "- vector search\n",
    "- index based retrieval\n",
    "- it can process multiple types of data (structured and unstructured)\n",
    "\n",
    "Modular RAG\n",
    "- any scenario that involves naive RAG, Advanced RAG, ML, or any complex project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How it is different from fine-tuning?\n",
    "\n",
    "RAG vs Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- RAG is not always an alternative to fine-tuning\n",
    "- fine-tuning cannot always replace RAG\n",
    "\n",
    "Fine-tuning\n",
    "- we cannot fine-tune a model with dynamic, ever changing data such as daily forecast\n",
    "\n",
    "\n",
    "whether to gor for RAG or fine-tune\n",
    "\n",
    "- parametric\n",
    "- non-paratmetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "we need balance between these 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Ecosystem\n",
    "\n",
    "<img src='r1.png' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data\n",
    "Storage\n",
    "Retrieval\n",
    "Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sqpsYn49QWSR"
   },
   "source": [
    "## 4. Implemention\n",
    "\n",
    "Naïve, Advanced, and Modular RAG through basic educational examples.\n",
    "\n",
    "**Part 1: Foundations and Basic Implementation**\n",
    "\n",
    "1.Environment setup for OpenAI API integration  \n",
    "2.Generator function using GPT models    \n",
    "3.Dataetup with a list of documents (db_records)  \n",
    "4.Query(user request)  \n",
    "\n",
    "**Part 2: Advanced Techniques and Evaluation**\n",
    "\n",
    "1.Retrieval metrics  \n",
    "2.Naïve RAG  \n",
    "3.Advanced RAG  \n",
    "4.Modular RAG Retriever  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ICSQQ0ipxlR"
   },
   "source": [
    "# Part 1: Foundations and Basic Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o01-IM8bTc5f"
   },
   "source": [
    "### 1.The Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8VCfbN0YwHbE",
    "outputId": "b0e19609-960a-46e5-d5cb-347177e198a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai==1.19.0 in /usr/local/lib/python3.10/dist-packages (1.19.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.19.0) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai==1.19.0) (1.7.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.19.0) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.19.0) (2.8.2)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai==1.19.0) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai==1.19.0) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai==1.19.0) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai==1.19.0) (3.7)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai==1.19.0) (1.2.2)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai==1.19.0) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai==1.19.0) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.19.0) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai==1.19.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai==1.19.0) (2.20.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai==1.40.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "myXJn33zbqTR",
    "outputId": "a3e945ff-0af0-412f-a1c3-9b4a708091cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "#API Key\n",
    "#Store you key in a file and read it(you can type it directly in the notebook but it will be visible for somebody next to you)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oefvqp21Ba07"
   },
   "outputs": [],
   "source": [
    "f = open(\"drive/MyDrive/files/api_key.txt\", \"r\")\n",
    "API_KEY=f.readline().strip()\n",
    "f.close()\n",
    "\n",
    "#The OpenAI Key\n",
    "import os\n",
    "import openai\n",
    "os.environ['OPENAI_API_KEY'] =API_KEY\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WuZ7jr4Rs36U"
   },
   "source": [
    "### 2.The Generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qwCNTW9fs36U"
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "import time\n",
    "\n",
    "client = OpenAI()\n",
    "gptmodel=\"gpt-4o\"\n",
    "start_time = time.time()  # Start timing before the request\n",
    "\n",
    "def call_llm_with_full_text(itext):\n",
    "    # Join all lines to form a single string\n",
    "    text_input = '\\n'.join(itext)\n",
    "    prompt = f\"Please elaborate on the following content:\\n{text_input}\"\n",
    "\n",
    "    try:\n",
    "      response = client.chat.completions.create(\n",
    "         model=gptmodel,\n",
    "         messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert Natural Language Processing exercise expert.\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"1.You can explain read the input and answer in detail\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "         ],\n",
    "         temperature=0.1  # Add the temperature parameter here and other parameters you need\n",
    "        )\n",
    "      return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        return str(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bVKe9VF0HIHJ"
   },
   "source": [
    "## Formatted response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oG8I2Kb2HFhL"
   },
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "def print_formatted_response(response):\n",
    "    # Define the width for wrapping the text\n",
    "    wrapper = textwrap.TextWrapper(width=80)  # Set to 80 columns wide, but adjust as needed\n",
    "    wrapped_text = wrapper.fill(text=response)\n",
    "\n",
    "    # Print the formatted response with a header and footer\n",
    "    print(\"Response:\")\n",
    "    print(\"---------------\")\n",
    "    print(wrapped_text)\n",
    "    print(\"---------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qv1ExPiZdJRL"
   },
   "source": [
    " # 3.The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "45CFxG4Fgcju"
   },
   "outputs": [],
   "source": [
    "db_records = [\n",
    "    \"Retrieval Augmented Generation (RAG) represents a sophisticated hybrid approach in the field of artificial intelligence, particularly within the realm of natural language processing (NLP).\",\n",
    "    \"It innovatively combines the capabilities of neural network-based language models with retrieval systems to enhance the generation of text, making it more accurate, informative, and contextually relevant.\",\n",
    "    \"This methodology leverages the strengths of both generative and retrieval architectures to tackle complex tasks that require not only linguistic fluency but also factual correctness and depth of knowledge.\",\n",
    "    \"At the core of Retrieval Augmented Generation (RAG) is a generative model, typically a transformer-based neural network, similar to those used in models like GPT (Generative Pre-trained Transformer) or BERT (Bidirectional Encoder Representations from Transformers).\",\n",
    "    \"This component is responsible for producing coherent and contextually appropriate language outputs based on a mixture of input prompts and additional information fetched by the retrieval component.\",\n",
    "    \"Complementing the language model is the retrieval system, which is usually built on a database of documents or a corpus of texts.\",\n",
    "    \"This system uses techniques from information retrieval to find and fetch documents that are relevant to the input query or prompt.\",\n",
    "    \"The mechanism of relevance determination can range from simple keyword matching to more complex semantic search algorithms which interpret the meaning behind the query to find the best matches.\",\n",
    "    \"This component merges the outputs from the language model and the retrieval system.\",\n",
    "    \"It effectively synthesizes the raw data fetched by the retrieval system into the generative process of the language model.\",\n",
    "    \"The integrator ensures that the information from the retrieval system is seamlessly incorporated into the final text output, enhancing the model's ability to generate responses that are not only fluent and grammatically correct but also rich in factual details and context-specific nuances.\",\n",
    "    \"When a query or prompt is received, the system first processes it to understand the requirement or the context.\",\n",
    "    \"Based on the processed query, the retrieval system searches through its database to find relevant documents or information snippets.\",\n",
    "    \"This retrieval is guided by the similarity of content in the documents to the query, which can be determined through various techniques like vector embeddings or semantic similarity measures.\",\n",
    "    \"The retrieved documents are then fed into the language model.\",\n",
    "    \"In some implementations, this integration happens at the token level, where the model can access and incorporate specific pieces of information from the retrieved texts dynamically as it generates each part of the response.\",\n",
    "    \"The language model, now augmented with direct access to retrieved information, generates a response.\",\n",
    "    \"This response is not only influenced by the training of the model but also by the specific facts and details contained in the retrieved documents, making it more tailored and accurate.\",\n",
    "    \"By directly incorporating information from external sources, Retrieval Augmented Generation (RAG) models can produce responses that are more factual and relevant to the given query.\",\n",
    "    \"This is particularly useful in domains like medical advice, technical support, and other areas where precision and up-to-date knowledge are crucial.\",\n",
    "    \"Retrieval Augmented Generation (RAG) systems can dynamically adapt to new information since they retrieve data in real-time from their databases.\",\n",
    "    \"This allows them to remain current with the latest knowledge and trends without needing frequent retraining.\",\n",
    "    \"With access to a wide range of documents, Retrieval Augmented Generation (RAG) systems can provide detailed and nuanced answers that a standalone language model might not be capable of generating based solely on its pre-trained knowledge.\",\n",
    "    \"While Retrieval Augmented Generation (RAG) offers substantial benefits, it also comes with its challenges.\",\n",
    "    \"These include the complexity of integrating retrieval and generation systems, the computational overhead associated with real-time data retrieval, and the need for maintaining a large, up-to-date, and high-quality database of retrievable texts.\",\n",
    "    \"Furthermore, ensuring the relevance and accuracy of the retrieved information remains a significant challenge, as does managing the potential for introducing biases or errors from the external sources.\",\n",
    "    \"In summary, Retrieval Augmented Generation represents a significant advancement in the field of artificial intelligence, merging the best of retrieval-based and generative technologies to create systems that not only understand and generate natural language but also deeply comprehend and utilize the vast amounts of information available in textual form.\",\n",
    "    \"A RAG vector store is a database or dataset that contains vectorized data points.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FIQ7NK92g7EC",
    "outputId": "0144c60a-7a92-4dae-ea9e-7135bd6b6b3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval Augmented Generation (RAG) represents a sophisticated hybrid approach\n",
      "in the field of artificial intelligence, particularly within the realm of\n",
      "natural language processing (NLP). It innovatively combines the capabilities of\n",
      "neural network-based language models with retrieval systems to enhance the\n",
      "generation of text, making it more accurate, informative, and contextually\n",
      "relevant. This methodology leverages the strengths of both generative and\n",
      "retrieval architectures to tackle complex tasks that require not only linguistic\n",
      "fluency but also factual correctness and depth of knowledge. At the core of\n",
      "Retrieval Augmented Generation (RAG) is a generative model, typically a\n",
      "transformer-based neural network, similar to those used in models like GPT\n",
      "(Generative Pre-trained Transformer) or BERT (Bidirectional Encoder\n",
      "Representations from Transformers). This component is responsible for producing\n",
      "coherent and contextually appropriate language outputs based on a mixture of\n",
      "input prompts and additional information fetched by the retrieval component.\n",
      "Complementing the language model is the retrieval system, which is usually built\n",
      "on a database of documents or a corpus of texts. This system uses techniques\n",
      "from information retrieval to find and fetch documents that are relevant to the\n",
      "input query or prompt. The mechanism of relevance determination can range from\n",
      "simple keyword matching to more complex semantic search algorithms which\n",
      "interpret the meaning behind the query to find the best matches. This component\n",
      "merges the outputs from the language model and the retrieval system. It\n",
      "effectively synthesizes the raw data fetched by the retrieval system into the\n",
      "generative process of the language model. The integrator ensures that the\n",
      "information from the retrieval system is seamlessly incorporated into the final\n",
      "text output, enhancing the model's ability to generate responses that are not\n",
      "only fluent and grammatically correct but also rich in factual details and\n",
      "context-specific nuances. When a query or prompt is received, the system first\n",
      "processes it to understand the requirement or the context. Based on the\n",
      "processed query, the retrieval system searches through its database to find\n",
      "relevant documents or information snippets. This retrieval is guided by the\n",
      "similarity of content in the documents to the query, which can be determined\n",
      "through various techniques like vector embeddings or semantic similarity\n",
      "measures. The retrieved documents are then fed into the language model. In some\n",
      "implementations, this integration happens at the token level, where the model\n",
      "can access and incorporate specific pieces of information from the retrieved\n",
      "texts dynamically as it generates each part of the response. The language model,\n",
      "now augmented with direct access to retrieved information, generates a response.\n",
      "This response is not only influenced by the training of the model but also by\n",
      "the specific facts and details contained in the retrieved documents, making it\n",
      "more tailored and accurate. By directly incorporating information from external\n",
      "sources, Retrieval Augmented Generation (RAG) models can produce responses that\n",
      "are more factual and relevant to the given query. This is particularly useful in\n",
      "domains like medical advice, technical support, and other areas where precision\n",
      "and up-to-date knowledge are crucial. Retrieval Augmented Generation (RAG)\n",
      "systems can dynamically adapt to new information since they retrieve data in\n",
      "real-time from their databases. This allows them to remain current with the\n",
      "latest knowledge and trends without needing frequent retraining. With access to\n",
      "a wide range of documents, Retrieval Augmented Generation (RAG) systems can\n",
      "provide detailed and nuanced answers that a standalone language model might not\n",
      "be capable of generating based solely on its pre-trained knowledge. While\n",
      "Retrieval Augmented Generation (RAG) offers substantial benefits, it also comes\n",
      "with its challenges. These include the complexity of integrating retrieval and\n",
      "generation systems, the computational overhead associated with real-time data\n",
      "retrieval, and the need for maintaining a large, up-to-date, and high-quality\n",
      "database of retrievable texts. Furthermore, ensuring the relevance and accuracy\n",
      "of the retrieved information remains a significant challenge, as does managing\n",
      "the potential for introducing biases or errors from the external sources. In\n",
      "summary, Retrieval Augmented Generation represents a significant advancement in\n",
      "the field of artificial intelligence, merging the best of retrieval-based and\n",
      "generative technologies to create systems that not only understand and generate\n",
      "natural language but also deeply comprehend and utilize the vast amounts of\n",
      "information available in textual form. A RAG vector store is a database or\n",
      "dataset that contains vectorized data points.\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "paragraph = ' '.join(db_records)\n",
    "wrapped_text = textwrap.fill(paragraph, width=80)\n",
    "print(wrapped_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aL7cHuuLhQ5w"
   },
   "source": [
    "### 4.The Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qARk6gtohSXW"
   },
   "outputs": [],
   "source": [
    "query = \"define a rag store\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iITo3QIF7yeK"
   },
   "source": [
    "Generation without augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TXBILWI47yeM",
    "outputId": "30f9fecf-609e-4ba1-f9f1-4168ba4a7760"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "---------------\n",
      "Certainly! The content you've provided appears to be a sequence of characters\n",
      "that, when combined, form the phrase \"define a rag store.\" Let's break this down\n",
      "step by step:  1. **Characters and Formation**:    - The sequence of characters\n",
      "is: d, e, f, i, n, e, a, r, a, g, s, t, o, r, e.    - When these characters are\n",
      "combined in the given order, they form the phrase \"define a rag store.\"  2.\n",
      "**Phrase Breakdown**:    - **Define**: This is a verb that means to explain the\n",
      "meaning of a word or concept.    - **A**: This is an indefinite article used\n",
      "before words that begin with a consonant sound.    - **Rag**: This is a noun\n",
      "that typically refers to a piece of old, often torn, cloth.    - **Store**: This\n",
      "is a noun that refers to a place where goods are sold to the public.  3.\n",
      "**Combined Meaning**:    - When put together, \"define a rag store\" is a request\n",
      "or instruction to explain what a \"rag store\" is.  4. **Interpreting \"Rag\n",
      "Store\"**:    - A \"rag store\" could be interpreted in a few ways:      - It could\n",
      "be a store that sells rags or old clothes.      - It might be a colloquial term\n",
      "for a thrift store or second-hand shop where used clothing and other items are\n",
      "sold.      - In some contexts, it could refer to a store that sells cleaning\n",
      "supplies, including rags.  5. **Contextual Usage**:    - The phrase \"define a\n",
      "rag store\" might be used in a variety of contexts, such as:      - A teacher\n",
      "asking a student to explain what a rag store is.      - A person seeking\n",
      "clarification on the term in a conversation.      - An instruction in a language\n",
      "learning exercise.  By breaking down the sequence of characters and\n",
      "understanding the individual components, we can interpret the phrase and its\n",
      "potential meanings.\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call the function and print the result\n",
    "llm_response = call_llm_with_full_text(query)\n",
    "print_formatted_response(llm_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "11HLkKQMqaDt"
   },
   "source": [
    "# Part 2: Advanced Techniques and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMGuZg1WiaUE"
   },
   "source": [
    "### 1.Retrieval Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KHN6s7wZirQL"
   },
   "source": [
    "#### Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_GLECrTQirQN"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def calculate_cosine_similarity(text1, text2):\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        stop_words='english',\n",
    "        use_idf=True,\n",
    "        norm='l2',\n",
    "        ngram_range=(1, 2),  # Use unigrams and bigrams\n",
    "        sublinear_tf=True,   # Apply sublinear TF scaling\n",
    "        analyzer='word'      # You could also experiment with 'char' or 'char_wb' for character-level features\n",
    "    )\n",
    "    tfidf = vectorizer.fit_transform([text1, text2])\n",
    "    similarity = cosine_similarity(tfidf[0:1], tfidf[1:2])\n",
    "    return similarity[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YTJOi-jrjI5A"
   },
   "source": [
    "#### Enhanced Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cnmPG9UWjJXD",
    "outputId": "0a49b32d-baa4-433f-e63f-0b9e8282a271"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "    return synonyms\n",
    "\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text.lower())\n",
    "    lemmatized_words = []\n",
    "    for token in doc:\n",
    "        if token.is_stop or token.is_punct:\n",
    "            continue\n",
    "        lemmatized_words.append(token.lemma_)\n",
    "    return lemmatized_words\n",
    "\n",
    "def expand_with_synonyms(words):\n",
    "    expanded_words = words.copy()\n",
    "    for word in words:\n",
    "        expanded_words.extend(get_synonyms(word))\n",
    "    return expanded_words\n",
    "\n",
    "def calculate_enhanced_similarity(text1, text2):\n",
    "    # Preprocess and tokenize texts\n",
    "    words1 = preprocess_text(text1)\n",
    "    words2 = preprocess_text(text2)\n",
    "\n",
    "    # Expand with synonyms\n",
    "    words1_expanded = expand_with_synonyms(words1)\n",
    "    words2_expanded = expand_with_synonyms(words2)\n",
    "\n",
    "    # Count word frequencies\n",
    "    freq1 = Counter(words1_expanded)\n",
    "    freq2 = Counter(words2_expanded)\n",
    "\n",
    "    # Create a set of all unique words\n",
    "    unique_words = set(freq1.keys()).union(set(freq2.keys()))\n",
    "\n",
    "    # Create frequency vectors\n",
    "    vector1 = [freq1[word] for word in unique_words]\n",
    "    vector2 = [freq2[word] for word in unique_words]\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    vector1 = np.array(vector1)\n",
    "    vector2 = np.array(vector2)\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    cosine_similarity = np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))\n",
    "\n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JFqh9rr81SUn"
   },
   "source": [
    "### 2.Naïve RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wu8vteKmS_qO"
   },
   "source": [
    "#### Keyword search and matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WY1JU0Ush_l4",
    "outputId": "b0fe74e3-a7e0-4095-ed79-0aaff88f429c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Keyword Score: 3\n",
      "Response:\n",
      "---------------\n",
      "A RAG vector store is a database or dataset that contains vectorized data\n",
      "points.\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def find_best_match_keyword_search(query, db_records):\n",
    "    best_score = 0\n",
    "    best_record = None\n",
    "\n",
    "    # Split the query into individual keywords\n",
    "    query_keywords = set(query.lower().split())\n",
    "\n",
    "    # Iterate through each record in db_records\n",
    "    for record in db_records:\n",
    "        # Split the record into keywords\n",
    "        record_keywords = set(record.lower().split())\n",
    "\n",
    "        # Calculate the number of common keywords\n",
    "        common_keywords = query_keywords.intersection(record_keywords)\n",
    "        current_score = len(common_keywords)\n",
    "\n",
    "        # Update the best score and record if the current score is higher\n",
    "        if current_score > best_score:\n",
    "            best_score = current_score\n",
    "            best_record = record\n",
    "\n",
    "    return best_score, best_record\n",
    "\n",
    "# Assuming 'query' and 'db_records' are defined in previous cells in your Colab notebook\n",
    "best_keyword_score, best_matching_record = find_best_match_keyword_search(query, db_records)\n",
    "\n",
    "print(f\"Best Keyword Score: {best_keyword_score}\")\n",
    "print_formatted_response(best_matching_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oak-3k_dkzC3"
   },
   "source": [
    "#### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "blcPOIaHkzC4",
    "outputId": "405b5388-5cea-4c1c-d6f9-8c6c4d92158c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Cosine Similarity Score: 0.126\n"
     ]
    }
   ],
   "source": [
    "# Cosine Similarity\n",
    "score = calculate_cosine_similarity(query, best_matching_record)\n",
    "print(f\"Best Cosine Similarity Score: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1OW0l24IkzC5",
    "outputId": "5ca7c128-2519-4852-cdc5-e9c3759c9a82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "define a rag store :  A RAG vector store is a database or dataset that contains vectorized data points.\n",
      "Enhanced Similarity:, 0.642\n"
     ]
    }
   ],
   "source": [
    "# Enhanced Similarity\n",
    "response = best_matching_record\n",
    "print(query,\": \", response)\n",
    "similarity_score = calculate_enhanced_similarity(query, response)\n",
    "print(f\"Enhanced Similarity:, {similarity_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r2zKQhiO0Fcr"
   },
   "source": [
    "#### Augmented input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r_7ymSxG0Fcs"
   },
   "outputs": [],
   "source": [
    "augmented_input=query+ \": \"+ best_matching_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7NTfxzum0PT2",
    "outputId": "05c75caa-af12-4aa2-e8d1-41f6cb02a312"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "---------------\n",
      "define a rag store: A RAG vector store is a database or dataset that contains\n",
      "vectorized data points.\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_formatted_response(augmented_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Ui8wH4k3_g4"
   },
   "source": [
    "#### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jh8BsnUy0Fcs",
    "outputId": "7d7cd617-cd81-4489-bbd3-93171beffe4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "---------------\n",
      "Certainly! Let's break down and elaborate on the provided content:  ### Define a\n",
      "RAG Store:  A **RAG (Retrieval-Augmented Generation) vector store** is a\n",
      "specialized type of database or dataset that is designed to store and manage\n",
      "vectorized data points.   #### Key Components:  1. **Vectorized Data Points**:\n",
      "- **Vectors**: In the context of machine learning and natural language\n",
      "processing, vectors are numerical representations of data. These vectors can\n",
      "represent words, sentences, images, or any other type of data in a high-\n",
      "dimensional space.    - **Vectorization**: This is the process of converting\n",
      "data into a vector format. For text, this might involve techniques like word\n",
      "embeddings (e.g., Word2Vec, GloVe) or sentence embeddings (e.g., BERT, Sentence-\n",
      "BERT).  2. **Database or Dataset**:    - **Database**: A structured collection\n",
      "of data that is stored and accessed electronically. In the case of a RAG vector\n",
      "store, the database is optimized for storing and retrieving vectorized data.\n",
      "- **Dataset**: A collection of data points that are organized in a specific\n",
      "manner. For a RAG vector store, the dataset would consist of vectorized\n",
      "representations of the data points.  #### Purpose and Usage:  - **Retrieval-\n",
      "Augmented Generation (RAG)**:   - **Retrieval**: The process of finding and\n",
      "fetching relevant data points from the vector store based on a query. This is\n",
      "typically done using similarity search techniques, where the query is also\n",
      "vectorized and compared with the stored vectors to find the closest matches.   -\n",
      "**Augmentation**: Enhancing the retrieved data with additional information or\n",
      "context. This can involve combining the retrieved vectors with other data or\n",
      "using them as input for further processing.   - **Generation**: Producing new\n",
      "content or responses based on the augmented data. In natural language\n",
      "processing, this might involve generating text that is informed by the retrieved\n",
      "and augmented data.  #### Applications:  - **Natural Language Processing\n",
      "(NLP)**: RAG vector stores are commonly used in NLP tasks such as question\n",
      "answering, text generation, and information retrieval. By storing vectorized\n",
      "representations of text, these systems can efficiently retrieve and generate\n",
      "relevant responses. - **Recommendation Systems**: Vector stores can be used to\n",
      "store user preferences and item features in a vectorized format, enabling\n",
      "efficient retrieval of recommendations based on similarity. - **Image and Video\n",
      "Retrieval**: Vectorized representations of images and videos can be stored in a\n",
      "RAG vector store, allowing for efficient retrieval based on visual similarity.\n",
      "### Summary:  A **RAG vector store** is a database or dataset that contains\n",
      "vectorized data points, enabling efficient retrieval, augmentation, and\n",
      "generation of information. It is particularly useful in applications that\n",
      "require handling and processing large amounts of vectorized data, such as\n",
      "natural language processing, recommendation systems, and multimedia retrieval.\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call the function and print the result\n",
    "llm_response = call_llm_with_full_text(augmented_input)\n",
    "print_formatted_response(llm_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zJH2__0iTUr1"
   },
   "source": [
    "## 3.Advanced RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "awyjcn35jFiy"
   },
   "source": [
    "### 3.1.Vector search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kD8_758kkq3h"
   },
   "source": [
    "### Search function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FCBbY4qLc8qh"
   },
   "outputs": [],
   "source": [
    "def find_best_match(text_input, records):\n",
    "    best_score = 0\n",
    "    best_record = None\n",
    "    for record in records:\n",
    "        current_score = calculate_cosine_similarity(text_input, record)\n",
    "        if current_score > best_score:\n",
    "            best_score = current_score\n",
    "            best_record = record\n",
    "    return best_score, best_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RG1iM-U33OCg"
   },
   "outputs": [],
   "source": [
    "best_similarity_score, best_matching_record = find_best_match(query, db_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PLa9NQ4Cm_YQ",
    "outputId": "15f15133-9cc5-4497-9f00-0d870f531547"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "---------------\n",
      "A RAG vector store is a database or dataset that contains vectorized data\n",
      "points.\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_formatted_response(best_matching_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A60QoOA3jf9j"
   },
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yIUh-38knHLI",
    "outputId": "d18d05cc-3fba-4be2-95a5-e98f8e12c8ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Cosine Similarity Score: 0.126\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best Cosine Similarity Score: {best_similarity_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PQopW_FSjBSr",
    "outputId": "480dd4d4-84db-4ce1-9b6c-4d90cec66803"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "define a rag store :  A RAG vector store is a database or dataset that contains vectorized data points.\n",
      "Enhanced Similarity:, 0.642\n"
     ]
    }
   ],
   "source": [
    "# Enhanced Similarity\n",
    "response = best_matching_record\n",
    "print(query,\": \", response)\n",
    "similarity_score = calculate_enhanced_similarity(query, best_matching_record)\n",
    "print(f\"Enhanced Similarity:, {similarity_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "51fZC6Oe2G9E"
   },
   "source": [
    "### Augmented input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4dcnK7OGx5e6"
   },
   "outputs": [],
   "source": [
    "augmented_input=query+\": \"+best_matching_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T3uk-91x049J",
    "outputId": "5542fef0-44a6-4687-a94a-68d84ea82482"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "---------------\n",
      "define a rag store: A RAG vector store is a database or dataset that contains\n",
      "vectorized data points.\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_formatted_response(augmented_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wFDF6hbi2LF9"
   },
   "source": [
    "### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YJC-mA5ftxFU",
    "outputId": "2edf23f9-82ef-445c-c627-7aaed0bf848b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "---------------\n",
      "Certainly! Let's break down and elaborate on the provided content:  **Define a\n",
      "RAG store:**  A **RAG vector store** is a database or dataset that contains\n",
      "vectorized data points.  ### Explanation:  1. **RAG**:    - RAG stands for\n",
      "**Retrieval-Augmented Generation**. It is a technique used in natural language\n",
      "processing (NLP) and machine learning where a model retrieves relevant\n",
      "information from a database or dataset to augment its generation capabilities.\n",
      "This helps in producing more accurate and contextually relevant responses or\n",
      "outputs.  2. **Vector Store**:    - A vector store is a specialized type of\n",
      "database or dataset designed to store and manage vectorized data points.\n",
      "Vectorized data points are numerical representations of data, often derived from\n",
      "text, images, or other forms of raw data through processes like embedding or\n",
      "feature extraction.  3. **Vectorized Data Points**:    - These are data points\n",
      "that have been transformed into vectors, which are arrays of numbers. This\n",
      "transformation is typically done using techniques such as word embeddings (e.g.,\n",
      "Word2Vec, GloVe), sentence embeddings, or other feature extraction methods. The\n",
      "purpose of vectorization is to convert complex data into a numerical format that\n",
      "can be easily processed by machine learning algorithms.  ### Use Case:  In the\n",
      "context of a RAG store, the vectorized data points are used to enhance the\n",
      "retrieval process. When a query is made, the system can quickly find the most\n",
      "relevant data points by comparing the query vector to the stored vectors. This\n",
      "retrieval process helps in generating more accurate and contextually appropriate\n",
      "responses or outputs.  ### Example:  Imagine you have a large collection of\n",
      "documents, and you want to build a system that can answer questions based on the\n",
      "information in those documents. By vectorizing the documents and storing them in\n",
      "a RAG vector store, you can efficiently retrieve the most relevant documents\n",
      "when a question is asked. The retrieved documents can then be used to generate a\n",
      "precise and informative answer.  In summary, a RAG vector store is a powerful\n",
      "tool in NLP and machine learning that combines the strengths of retrieval and\n",
      "generation techniques to improve the accuracy and relevance of generated\n",
      "outputs.\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call the function and print the result\n",
    "llm_response = call_llm_with_full_text(augmented_input)\n",
    "print_formatted_response(llm_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t7djpPBpm0M2"
   },
   "source": [
    "## 3.2.Index-based search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WyDUhy_1lBfT"
   },
   "source": [
    "### Search Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wRarT_fym2XC",
    "outputId": "10de2325-bbbd-4f16-fe10-aa984d29a55b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "---------------\n",
      "A RAG vector store is a database or dataset that contains vectorized data\n",
      "points.\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def setup_vectorizer(records):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(records)\n",
    "    return vectorizer, tfidf_matrix\n",
    "\n",
    "def find_best_match(query, vectorizer, tfidf_matrix):\n",
    "    query_tfidf = vectorizer.transform([query])\n",
    "    similarities = cosine_similarity(query_tfidf, tfidf_matrix)\n",
    "    best_index = similarities.argmax()  # Get the index of the highest similarity score\n",
    "    best_score = similarities[0, best_index]\n",
    "    return best_score, best_index\n",
    "\n",
    "vectorizer, tfidf_matrix = setup_vectorizer(db_records)\n",
    "\n",
    "best_similarity_score, best_index = find_best_match(query, vectorizer, tfidf_matrix)\n",
    "best_matching_record = db_records[best_index]\n",
    "\n",
    "print_formatted_response(best_matching_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jt3iBtJFj4sa"
   },
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UoNUQqx5j3r4",
    "outputId": "305f1c40-56a9-4b86-8095-8b893eed008e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Cosine Similarity Score: 0.407\n",
      "Response:\n",
      "---------------\n",
      "A RAG vector store is a database or dataset that contains vectorized data\n",
      "points.\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cosine Similarity\n",
    "print(f\"Best Cosine Similarity Score: {best_similarity_score:.3f}\")\n",
    "print_formatted_response(best_matching_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vg910Mhuj3sO",
    "outputId": "55606de1-6534-46e1-920d-975c7181e4e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "define a rag store :  A RAG vector store is a database or dataset that contains vectorized data points.\n",
      "Enhanced Similarity:, 0.642\n"
     ]
    }
   ],
   "source": [
    "# Enhanced Similarity\n",
    "response = best_matching_record\n",
    "print(query,\": \", response)\n",
    "similarity_score = calculate_enhanced_similarity(query, response)\n",
    "print(f\"Enhanced Similarity:, {similarity_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ubm0DTxKeqR9"
   },
   "source": [
    "Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SbokQ2eacHjM",
    "outputId": "e8926ae2-7195-4ad1-9001-7a9c7e64cf18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ability    access  accuracy  accurate     adapt  additional  advancement  \\\n",
      "0   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "1   0.000000  0.000000  0.000000  0.216364  0.000000    0.000000     0.000000   \n",
      "2   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "3   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "4   0.000000  0.000000  0.000000  0.000000  0.000000    0.236479     0.000000   \n",
      "5   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "6   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "7   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "8   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "9   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "10  0.186734  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "11  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "12  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "13  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "14  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "15  0.000000  0.172624  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "16  0.000000  0.317970  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "17  0.000000  0.000000  0.000000  0.206861  0.000000    0.000000     0.000000   \n",
      "18  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "19  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "20  0.000000  0.000000  0.000000  0.000000  0.275802    0.000000     0.000000   \n",
      "21  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "22  0.000000  0.174772  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "23  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "24  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "25  0.000000  0.000000  0.228743  0.000000  0.000000    0.000000     0.000000   \n",
      "26  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.173327   \n",
      "27  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "\n",
      "      advice  algorithms    allows  ...    vector  vectorized      when  \\\n",
      "0   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "1   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "2   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "3   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "4   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "5   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "6   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "7   0.000000    0.220687  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "8   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "9   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "10  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "11  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.295573   \n",
      "12  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "13  0.000000    0.000000  0.000000  ...  0.200131     0.00000  0.000000   \n",
      "14  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "15  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "16  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "17  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "18  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "19  0.244401    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "20  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "21  0.000000    0.000000  0.291503  ...  0.000000     0.00000  0.000000   \n",
      "22  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "23  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "24  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "25  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "26  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "27  0.000000    0.000000  0.000000  ...  0.307719     0.34589  0.000000   \n",
      "\n",
      "       where     which    while     wide      with    within   without  \n",
      "0   0.000000  0.000000  0.00000  0.00000  0.000000  0.260582  0.000000  \n",
      "1   0.000000  0.000000  0.00000  0.00000  0.160278  0.000000  0.000000  \n",
      "2   0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "3   0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "4   0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "5   0.000000  0.247710  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "6   0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "7   0.000000  0.179053  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "8   0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "9   0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "10  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "11  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "12  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "13  0.000000  0.182517  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "14  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "15  0.189283  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "16  0.000000  0.000000  0.00000  0.00000  0.258278  0.000000  0.000000  \n",
      "17  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "18  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "19  0.217430  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "20  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "21  0.000000  0.000000  0.00000  0.00000  0.192110  0.000000  0.291503  \n",
      "22  0.000000  0.000000  0.00000  0.21541  0.141963  0.000000  0.000000  \n",
      "23  0.000000  0.000000  0.32932  0.00000  0.217033  0.000000  0.000000  \n",
      "24  0.000000  0.000000  0.00000  0.00000  0.134513  0.000000  0.000000  \n",
      "25  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "26  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "27  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "\n",
      "[28 rows x 297 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def setup_vectorizer(records):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(records)\n",
    "\n",
    "    # Convert the TF-IDF matrix to a DataFrame for display purposes\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "    # Display the DataFrame\n",
    "    print(tfidf_df)\n",
    "\n",
    "    return vectorizer, tfidf_matrix\n",
    "\n",
    "vectorizer, tfidf_matrix = setup_vectorizer(db_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dABZ12Bkugtt"
   },
   "source": [
    "### Augmented input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1w4wppuA4eNn"
   },
   "outputs": [],
   "source": [
    "augmented_input=query+\": \"+best_matching_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MNozI65K4e7u",
    "outputId": "d3a894fd-fa75-43f0-8291-6472671c88f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "---------------\n",
      "define a rag store: A RAG vector store is a database or dataset that contains\n",
      "vectorized data points.\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_formatted_response(augmented_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hU998zkD4hpD"
   },
   "source": [
    "### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uy9X-l_Iugtt",
    "outputId": "c5819c7f-b236-4f02-ab40-7986808498e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "---------------\n",
      "Certainly! Let's break down and elaborate on the given content:  ---  **Define a\n",
      "RAG store:**  A **RAG vector store** is a **database** or **dataset** that\n",
      "contains **vectorized data points**.  ---  ### Detailed Explanation:  1. **RAG\n",
      "Store**:    - **RAG** stands for **Retrieval-Augmented Generation**. It is a\n",
      "technique used in natural language processing (NLP) where a model retrieves\n",
      "relevant information from a database or dataset to augment its generation\n",
      "capabilities.    - A **RAG store** is essentially a storage system designed to\n",
      "hold and manage the data used in this retrieval process.  2. **Vector Store**:\n",
      "- A **vector store** is a specialized type of database that stores data in the\n",
      "form of vectors. Vectors are mathematical representations of data points, often\n",
      "used in machine learning and NLP to represent words, sentences, or other types\n",
      "of data in a high-dimensional space.    - These vectors are typically generated\n",
      "using techniques like word embeddings (e.g., Word2Vec, GloVe) or more advanced\n",
      "methods like transformer-based models (e.g., BERT, GPT).  3. **Database or\n",
      "Dataset**:    - The **database** or **dataset** in this context refers to the\n",
      "collection of vectorized data points. This can be a structured database like SQL\n",
      "or a more flexible dataset stored in formats like JSON, CSV, or specialized\n",
      "vector databases.  4. **Vectorized Data Points**:    - **Vectorized data\n",
      "points** are the core elements stored in the RAG vector store. Each data point\n",
      "is converted into a vector, which is a list of numerical values representing the\n",
      "data in a way that captures its semantic meaning.    - For example, in NLP,\n",
      "words or sentences are converted into vectors that capture their meanings and\n",
      "relationships to other words or sentences.  ### Use Case:  In a practical\n",
      "scenario, a RAG vector store might be used in a chatbot or a question-answering\n",
      "system. When a user asks a question, the system retrieves relevant information\n",
      "from the vector store to generate a more accurate and contextually appropriate\n",
      "response.  ### Example:  Imagine you have a large collection of documents. Each\n",
      "document is converted into a vector and stored in the RAG vector store. When a\n",
      "user asks a question, the system retrieves the most relevant document vectors\n",
      "and uses them to generate a response that incorporates information from those\n",
      "documents.  ---  By storing and managing vectorized data points, a RAG vector\n",
      "store enables efficient retrieval and augmentation of information, enhancing the\n",
      "capabilities of NLP models in generating accurate and contextually rich\n",
      "responses.\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call the function and print the result\n",
    "llm_response = call_llm_with_full_text(augmented_input)\n",
    "print_formatted_response(llm_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wWEvzcDHTX6i"
   },
   "source": [
    "# 4.Modular RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "18wmqwJd4o62"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class RetrievalComponent:\n",
    "    def __init__(self, method='vector'):\n",
    "        self.method = method\n",
    "        if self.method == 'vector' or self.method == 'indexed':\n",
    "            self.vectorizer = TfidfVectorizer()\n",
    "            self.tfidf_matrix = None\n",
    "\n",
    "    def fit(self, records):\n",
    "        if self.method == 'vector' or self.method == 'indexed':\n",
    "            self.tfidf_matrix = self.vectorizer.fit_transform(records)\n",
    "\n",
    "    def retrieve(self, query):\n",
    "        if self.method == 'keyword':\n",
    "            return self.keyword_search(query)\n",
    "        elif self.method == 'vector':\n",
    "            return self.vector_search(query)\n",
    "        elif self.method == 'indexed':\n",
    "            return self.indexed_search(query)\n",
    "\n",
    "    def keyword_search(self, query):\n",
    "        best_score = 0\n",
    "        best_record = None\n",
    "        query_keywords = set(query.lower().split())\n",
    "        for index, doc in enumerate(self.documents):\n",
    "            doc_keywords = set(doc.lower().split())\n",
    "            common_keywords = query_keywords.intersection(doc_keywords)\n",
    "            score = len(common_keywords)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_record = self.documents[index]\n",
    "        return best_record\n",
    "\n",
    "    def vector_search(self, query):\n",
    "        query_tfidf = self.vectorizer.transform([query])\n",
    "        similarities = cosine_similarity(query_tfidf, self.tfidf_matrix)\n",
    "        best_index = similarities.argmax()\n",
    "        return db_records[best_index]\n",
    "\n",
    "    def indexed_search(self, query):\n",
    "        # Assuming the tfidf_matrix is precomputed and stored\n",
    "        query_tfidf = self.vectorizer.transform([query])\n",
    "        similarities = cosine_similarity(query_tfidf, self.tfidf_matrix)\n",
    "        best_index = similarities.argmax()\n",
    "        return db_records[best_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7qHm4saJ8cGk"
   },
   "source": [
    "### Modular RAG Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_kvhIOdY8amp",
    "outputId": "33b5a118-6a55-4584-85da-db71af264092"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "---------------\n",
      "A RAG vector store is a database or dataset that contains vectorized data\n",
      "points.\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Usage example\n",
    "retrieval = RetrievalComponent(method='vector')  # Choose from 'keyword', 'vector', 'indexed'\n",
    "retrieval.fit(db_records)\n",
    "best_matching_record = retrieval.retrieve(query)\n",
    "\n",
    "print_formatted_response(best_matching_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DgxXdqzvkYDk"
   },
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "COyYme4IkYDx",
    "outputId": "a7673d23-5531-45b3-9042-731b39d3e58d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Cosine Similarity Score: 0.407\n",
      "Response:\n",
      "---------------\n",
      "A RAG vector store is a database or dataset that contains vectorized data\n",
      "points.\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cosine Similarity\n",
    "print(f\"Best Cosine Similarity Score: {best_similarity_score:.3f}\")\n",
    "print_formatted_response(best_matching_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0YRTpIpzkYDx",
    "outputId": "88e76717-2a04-4ffa-bf29-8eaa3f555b40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "define a rag store :  A RAG vector store is a database or dataset that contains vectorized data points.\n",
      "Enhanced Similarity: 0.641582812483307\n"
     ]
    }
   ],
   "source": [
    "# Enhanced Similarity\n",
    "response = best_matching_record\n",
    "print(query,\": \", response)\n",
    "similarity_score = calculate_enhanced_similarity(query, response)\n",
    "print(\"Enhanced Similarity:\", similarity_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5TaQa7Dc7JwT"
   },
   "source": [
    "### Augmented Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X-hKjhIU7Jwg"
   },
   "outputs": [],
   "source": [
    "augmented_input=query+ \" \"+ best_matching_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZhSO-fyZ7Jwg",
    "outputId": "9299af2f-ca06-4edc-8087-cf1a76f077c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "---------------\n",
      "define a rag store A RAG vector store is a database or dataset that contains\n",
      "vectorized data points.\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_formatted_response(augmented_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qkyYx_MC7Jwg"
   },
   "source": [
    "### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-V3srRHW7Jwh",
    "outputId": "29abaf2a-3793-4be4-a2df-cea50b6d30df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "---------------\n",
      "Certainly! Let's break down and elaborate on the content provided:  ---\n",
      "**Define a RAG store:**  A **RAG (Retrieval-Augmented Generation) store** is a\n",
      "specialized type of data storage system designed to support the retrieval and\n",
      "generation of information. It typically integrates two main components:  1.\n",
      "**Vector Store**: This is a database or dataset that contains vectorized data\n",
      "points. Vectorization is the process of converting data into a numerical format\n",
      "that can be easily processed by machine learning algorithms. In the context of a\n",
      "RAG store, these vectors often represent text, images, or other types of data in\n",
      "a high-dimensional space.  2. **Retrieval Mechanism**: This component is\n",
      "responsible for efficiently searching and retrieving relevant vectors from the\n",
      "vector store based on a given query. The retrieval mechanism often uses\n",
      "similarity measures (such as cosine similarity) to find vectors that are most\n",
      "similar to the query vector.  **Vector Store:**  A **vector store** is a\n",
      "database or dataset that contains vectorized data points. Here’s a more detailed\n",
      "explanation:  - **Database or Dataset**: This refers to a structured collection\n",
      "of data. In the case of a vector store, the data is stored in the form of\n",
      "vectors.    - **Vectorized Data Points**: These are data points that have been\n",
      "transformed into vectors. A vector is a mathematical representation of data in a\n",
      "multi-dimensional space. For example, in natural language processing (NLP),\n",
      "words or sentences can be converted into vectors using techniques like Word2Vec,\n",
      "GloVe, or BERT embeddings. Each vector captures the semantic meaning of the data\n",
      "point in a way that can be processed by algorithms.  **Purpose and Use Cases:**\n",
      "- **Information Retrieval**: Vector stores are used to quickly retrieve relevant\n",
      "information based on a query. For example, in a search engine, a user’s query\n",
      "can be vectorized and compared against vectors in the store to find the most\n",
      "relevant documents.  - **Machine Learning and AI**: Vector stores are essential\n",
      "for various machine learning and AI applications, including recommendation\n",
      "systems, natural language understanding, and image recognition. They enable\n",
      "efficient similarity searches and clustering of data points.  - **Scalability**:\n",
      "Vector stores are designed to handle large volumes of data and support fast\n",
      "retrieval operations, making them suitable for real-time applications.  In\n",
      "summary, a RAG store combines the capabilities of a vector store with retrieval\n",
      "mechanisms to support advanced information retrieval and generation tasks. The\n",
      "vector store component is crucial as it holds the vectorized representations of\n",
      "data, enabling efficient and accurate retrieval of relevant information.\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call the function and print the result\n",
    "llm_response = call_llm_with_full_text(augmented_input)\n",
    "print_formatted_response(llm_response)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOUkqLHEU8Hk8m8V1qL3HKy",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
