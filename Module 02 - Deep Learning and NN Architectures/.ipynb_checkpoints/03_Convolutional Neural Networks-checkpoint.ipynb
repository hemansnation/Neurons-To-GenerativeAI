{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6929941",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdd68ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "constrains\n",
    "\n",
    "- you need huge data(100k samples - tabular data)\n",
    "- unstructured dataset(images, videos, audios - greater than 50k samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016ef627",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANN\n",
    "\n",
    "1000x1000x3 = huge number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03dadcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# where is linear algebra used in daily life?\n",
    "# phone screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac8c818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# real world applications of CNN\n",
    "\n",
    "- face recognition - DCNN (Deep CNN)\n",
    "- image classification\n",
    "- object detection\n",
    "- medical image analysis - Xrays, MRIs\n",
    "- video analytics - action recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da931bfa",
   "metadata": {},
   "source": [
    "# Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dffc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "- convolution is a mathematical operation\n",
    "- the foundational part of CNN is a filter(matrix)\n",
    "\n",
    "1*1 + 6*0 + 9*(-1) + 2*1 ... = 18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d1e056",
   "metadata": {},
   "source": [
    "\n",
    "<img src='c1.png' />\n",
    "\n",
    "<br />\n",
    "\n",
    "<img src='c2.png' />\n",
    "\n",
    "<br />\n",
    "\n",
    "<img src='c4.png' />\n",
    "\n",
    "<br />\n",
    "\n",
    "<img src='c5.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710ea2a4",
   "metadata": {},
   "source": [
    "## what about colored image?\n",
    "\n",
    "<img src='c6.png' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bd0d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "6x6x3 * 3x3x3 = 4x4x3\n",
    "\n",
    "3 is reprenting RGB - Red, Green, Blue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebf87c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Convolutional Layer\n",
    "\n",
    "- filters(kernels)\n",
    "- padding\n",
    "- strides"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d842750",
   "metadata": {},
   "source": [
    "# Padding\n",
    "\n",
    "<img src='c7.png' />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655ea485",
   "metadata": {},
   "outputs": [],
   "source": [
    "in the convolutional operation the corner pixel will be used only once as compared to other pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5f9732",
   "metadata": {},
   "source": [
    "<img src='c8.png' />\n",
    "\n",
    "# Apply padding\n",
    "\n",
    "<img src='c9.png' />\n",
    "\n",
    "# results\n",
    "\n",
    "<img src='c10.png' />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62dc3cb",
   "metadata": {},
   "source": [
    "# Stride\n",
    "\n",
    "<img src='c11.png' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5176e17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stride = 1\n",
    "- the filter will move 1 pixel at a time\n",
    "\n",
    "stride = 2\n",
    "- the filter will move 2 pixels at a time\n",
    "- reduce the dimensions of the output image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3c2961",
   "metadata": {},
   "outputs": [],
   "source": [
    "when you pass the image through the convolutional layer \n",
    "- the output will be the feature map that highlights the \n",
    "    presence of certain features within the image.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef5ba35",
   "metadata": {},
   "source": [
    "# pooling layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b192c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "used to reduce the dimensions of feature maps\n",
    "why:\n",
    "- computational load \n",
    "- the number of parameters\n",
    "\n",
    "2 types of pooling operation\n",
    "\n",
    "1 - max pooling\n",
    "2 - average pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd588f6",
   "metadata": {},
   "source": [
    "# Pooling\n",
    "\n",
    "<img src='c12.png' />\n",
    "\n",
    "# But Why\n",
    "\n",
    "<img src='c13.png' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce19a4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "the output of a pooling layer will be a downsampled(or subsample) feature map\n",
    "\n",
    "- hold the most important information while reducing the size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3a6b0c",
   "metadata": {},
   "source": [
    "# Fully Connected (Dense) Layer\n",
    "\n",
    "<img src='c14.png' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd3ea8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten the feature map into 1-D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc1e95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "in python if you do the matrix calculation you need nested loops\n",
    "\n",
    "for nested loop the complexity will be n-square\n",
    "\n",
    "for i in range(1,4):\n",
    "    for j in some(1,4):\n",
    "        \n",
    "        \n",
    "12 - iteration value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf37524",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation function\n",
    "- sigmoid\n",
    "- softmax - we use this because it has less complex math equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d1d239",
   "metadata": {},
   "outputs": [],
   "source": [
    "in convolutional layer we use ReLU Activation function\n",
    "\n",
    "- it will return 0 for each negative value\n",
    "- handle non-linearities better\n",
    "\n",
    "\n",
    "the output is an activated feature map where non-important features are suppressed(values set to zero)\n",
    "and important features are highlighted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54064a40",
   "metadata": {},
   "source": [
    "# CNN Architecture\n",
    "\n",
    "<img src='c15.png' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e7a927",
   "metadata": {},
   "outputs": [],
   "source": [
    "input\n",
    "convolutional layer\n",
    "    - filter\n",
    "    - pooling\n",
    "    - stride\n",
    "    - ReLU activation function\n",
    "pooling layer\n",
    "    - reduce the size of feature map\n",
    "    - 1 - max pooling\n",
    "    - 2 - average pooling\n",
    "    - output will be downsample of the feature map\n",
    "fully connected dense layer\n",
    "    - to perform classification or regression\n",
    "    - softmax activation to generate probabilities\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69a0e50",
   "metadata": {},
   "source": [
    "<img src='c16.png' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9143e967",
   "metadata": {},
   "outputs": [],
   "source": [
    "backpropagation and training\n",
    "\n",
    "- in training the network learn by adjusting its filters and weights\n",
    "\n",
    "- loss function\n",
    "    - the difference between the predicted output and the true label\n",
    "    - cross-entropy loss\n",
    "- backpropagation\n",
    "    - the error will be propagated back to the network\n",
    "    - the filter and weights will be updated\n",
    "    - SGD (Stochastic Gradient Descent) or Adam (optimization algorithms)\n",
    "- learning rate\n",
    "    - a hyperparameter that controls the size of the steps taken during the optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc71fa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2777abff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d4c8ad3",
   "metadata": {},
   "source": [
    "# Resources and Code\n",
    "\n",
    "\n",
    "https://cs231n.github.io/convolutional-networks\n",
    "\n",
    "\n",
    "MIT - https://www.youtube.com/watch?v=NmLK_WQBxB4\n",
    "\n",
    "\n",
    "Code:\n",
    "https://blog.paperspace.com/writing-cnns-from-scratch-in-pytorch/\n",
    "\n",
    "\n",
    "ReLU vs Sigmoid\n",
    "\n",
    "https://wandb.ai/ayush-thakur/dl-question-bank/reports/ReLU-vs-Sigmoid-Function-in-Deep-Neural-Networks--VmlldzoyMDk0MzI\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ce7019",
   "metadata": {},
   "source": [
    "# CNN with CIFAR - 10 Dataset\n",
    "\n",
    "https://www.cs.toronto.edu/~kriz/cifar.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4c0806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all required imports\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# variables\n",
    "batch_size = 64\n",
    "num_classes = 10\n",
    "learning_rate = 0.001\n",
    "num_epochs = 20\n",
    "\n",
    "# where do you want to run your model? on GPU or CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c834bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "\n",
    "all_transforms = transforms.Compose([transforms.Resize((32,32)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                     transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],\n",
    "                                                         std=[0.2023, 0.1994, 0.2010])\n",
    "                                    ])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data',\n",
    "                                            train=True,\n",
    "                                            transform=all_transforms,\n",
    "                                            download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data',\n",
    "                                           train=False,\n",
    "                                           transform=all_transforms,\n",
    "                                           download=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9542ea66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doubt of generating multiple channels through convolution operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1255db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN\n",
    "\n",
    "class ConvNeuralNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ConvNeuralNet, self).__init__()\n",
    "        \n",
    "        self.conv_layer1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3)\n",
    "        self.conv_layer2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3)\n",
    "        self.max_pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv_layer3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)\n",
    "        self.conv_layer4 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3)\n",
    "        self.max_pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(1600,128)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128,num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv_layer1(x)\n",
    "        out = self.conv_layer2(out)\n",
    "        out = self.max_pool1(out)\n",
    "        \n",
    "        out = self.conv_layer3(out)\n",
    "        out = self.conv_layer4(out)\n",
    "        out = self.max_pool2(out)\n",
    "        \n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        \n",
    "        out = self.fc1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d110662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "\n",
    "model = ConvNeuralNet(num_classes)\n",
    "\n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, \n",
    "                            weight_decay=0.005, momentum=0.9)\n",
    "\n",
    "total_step = len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecb0281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trianing\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # backward pass and optimizer\n",
    "        optimizer.zero_grad()  #set the gradient to zero before every update\n",
    "        loss.backward()        # calculate the new gradient\n",
    "        optimizer.step()       # update the weights\n",
    "    \n",
    "    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d019707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data,1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "    print(\"Accuracy on {} train images = {}%\".format(50000, 100*correct/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78251f10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
