{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac607d93",
   "metadata": {},
   "source": [
    "# Encoder Decoder\n",
    "\n",
    "<img src='e1.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b11c49",
   "metadata": {},
   "source": [
    "seq 2 seq learning with neural network\n",
    "\n",
    "research paper: https://arxiv.org/abs/1409.3215\n",
    "        \n",
    "- how to efficiently map seq of variable lengths from one domain \n",
    "    to seq of variable lengths in another domain\n",
    "    \n",
    "model architecture\n",
    "- encoder\n",
    "- decoder\n",
    "- seq2seq framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff08348",
   "metadata": {},
   "outputs": [],
   "source": [
    "variants of encoder decoder architectures\n",
    "\n",
    "- seq 2 seq with attention\n",
    "- transformers\n",
    "- conditional variational autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0a0f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "application\n",
    "\n",
    "- machine translation - google translate - seq2seq (LSTM or GRU or transformer)\n",
    "- text summarization - news papers - encoder-decoder with attention\n",
    "                                   - BART - Bidirectional and Auto-Regressive Transformer\n",
    "\n",
    "- image captioning - social media, photo management - encoder(CNN like ResNet) \n",
    "                                                    - decoder (LSTM or Transformer)\n",
    "\n",
    "- speech recognition - alexa, siri - encoder - audio frames (CNN or LSTM)\n",
    "                                   - decoder - generate text\n",
    "\n",
    "    \n",
    "encoder-decoder used in transformers\n",
    "- BERT\n",
    "- GPT\n",
    "- T5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16a75ce",
   "metadata": {},
   "source": [
    "## encoder decoder architecture\n",
    "\n",
    "<img src='e2.png' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd42a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder\n",
    "- it process the input word by word\n",
    "- each word is represented as a vector(word embeddings)\n",
    "- it is based on LSTM or GRU\n",
    "- takes the word vector sequentially and update its hidden state\n",
    "- the final hidden state will also called as context vector\n",
    "- context vector will be the output or encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97229a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder\n",
    "- it takes the context vector as the input\n",
    "- it generates translated output seq word by word\n",
    "- LSTM, GRU or Transformer layers\n",
    "- it will use context vector provided by encoder as well as its own hidden state from previous step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063b52af",
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - input seq encoding - encoder \n",
    "    - process it word by word/token \n",
    "    - update hidden state \n",
    "    - produce context vector\n",
    "\n",
    "2 - context vector\n",
    "    - compressed representation of input seq\n",
    "    - more advanced models with attention mechanism and allow the decoder to acess all hidden states \n",
    "        of the encoder not jsut a single context vector\n",
    "\n",
    "3 - decoding the output seq\n",
    "    - at each step it produces 1 element of the output and updates its own hidden state\n",
    "    - it will continue until a special token(end-of-sequence token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ede476",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b9862b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "997523ec",
   "metadata": {},
   "source": [
    "## On example dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e4c3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "input_size = 10\n",
    "hidden_size = 128\n",
    "output_size = 10\n",
    "num_layers = 2\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "num_epochs = 20\n",
    "seq_length = 10\n",
    "\n",
    "# Dataset: Sequences of integers (e.g., [1, 2, 3] -> [3, 2, 1])\n",
    "class ReverseSequenceDataset(Dataset):\n",
    "    def __init__(self, num_samples, seq_length):\n",
    "        self.num_samples = num_samples\n",
    "        self.seq_length = seq_length\n",
    "        self.data = torch.randint(1, 10, (num_samples, seq_length))\n",
    "        self.targets = torch.flip(self.data, dims=[1])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.targets[index]\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, (hidden, cell) = self.lstm(x, (h0, c0))\n",
    "        return hidden, cell\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, num_layers):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(output_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        out, (hidden, cell) = self.lstm(x, (hidden, cell))\n",
    "        out = self.fc(out)\n",
    "        return out, hidden, cell\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, seq_length):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def forward(self, src, target):\n",
    "        batch_size = src.size(0)\n",
    "        target_length = target.size(1)\n",
    "        outputs = torch.zeros(batch_size, target_length, output_size).to(src.device)\n",
    "\n",
    "        hidden, cell = self.encoder(src)\n",
    "\n",
    "        decoder_input = target[:, 0].unsqueeze(1)\n",
    "\n",
    "        for t in range(1, target_length):\n",
    "            output, hidden, cell = self.decoder(decoder_input, hidden, cell)\n",
    "            outputs[:, t, :] = output.squeeze(1)\n",
    "            decoder_input = output  # the next input is the current output\n",
    "\n",
    "        return outputs\n",
    "\n",
    "dataset = ReverseSequenceDataset(num_samples=1000, seq_length=seq_length)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "encoder = Encoder(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers)\n",
    "decoder = Decoder(hidden_size=hidden_size, output_size=output_size, num_layers=num_layers)\n",
    "model = Seq2Seq(encoder, decoder, seq_length=seq_length).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for src, target in dataloader:\n",
    "        src = src.float().unsqueeze(-1).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        target = target.long().to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        outputs = model(src, target)\n",
    "        outputs = outputs.view(-1, output_size)\n",
    "        target = target.view(-1)\n",
    "\n",
    "        loss = criterion(outputs, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "example_src, example_target = dataset[0]\n",
    "example_src = example_src.float().unsqueeze(0).unsqueeze(-1).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "predicted_output = model(example_src, example_target.unsqueeze(0).to('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "\n",
    "print(f'Input Sequence: {example_src.squeeze(0).squeeze(-1).cpu().numpy()}')\n",
    "print(f'Reversed Target: {example_target.cpu().numpy()}')\n",
    "print(f'Predicted Output: {torch.argmax(predicted_output.squeeze(0), dim=1).cpu().numpy()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9a19cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "556a5a31",
   "metadata": {},
   "source": [
    "# Attention Mechanism\n",
    "\n",
    "<img src='a1.png' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b3ce6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "selective focus process\n",
    "\n",
    "attention solve these 2 challenges -\n",
    "- long sequence\n",
    "- contextual understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedfa94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Himanshu is taking a session. He is going to explain transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3008dc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "Himanshu -> He"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20ca632",
   "metadata": {},
   "outputs": [],
   "source": [
    "types of attention\n",
    "\n",
    "- self attention - attention within a sentence\n",
    "                 - consider relationships between different parts of the same sentence.\n",
    "    \n",
    "- scaled dot product attention\n",
    "    - dot product of query and key\n",
    "\n",
    "- location based attention\n",
    "    - used in image related tasks\n",
    "    - assign weights based on location of the element in the input\n",
    "    - object detection and image captioninig"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
